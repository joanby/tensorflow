{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distancia de Levenshtein"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x,y\\in S$ donde $S$ es el conjunto de Strings, entonces $d(x,y)$ se define como el número de operaciones (ediciones) para convertir la palabra $x$ en la palabra $y$, donde las ediciones posibles de la palabra son:\n",
    "- insertar un nuevo caracter\n",
    "- eliminar un caracter de la palabra\n",
    "- sustituir un caracter por otro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "session = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = list('casa')\n",
    "truth = list('calle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1 = tf.SparseTensor([[0,0,0], [0,0,1], [0,0,2], [0,0,3]],\n",
    "                    hypothesis, [1,1,1])\n",
    "t1 = tf.SparseTensor([[0,0,0], [0,0,1], [0,0,2], [0,0,3], [0,0,4]],\n",
    "                    truth, [1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.]]\n"
     ]
    }
   ],
   "source": [
    "print(session.run(tf.edit_distance(h1, t1, normalize=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.6]]\n"
     ]
    }
   ],
   "source": [
    "print(session.run(tf.edit_distance(h1, t1, normalize=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseTensorValue(indices=array([[0, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 2],\n",
       "       [0, 0, 3]]), values=array([b'c', b'a', b's', b'a'], dtype=object), dense_shape=array([1, 1, 1]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis2 = list('casacalle')\n",
    "truth2 = list('callescalles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2 = tf.SparseTensor([[0,0,0], [0,0,1], [0,0,2], [0,0,3], [0,1,0], [0,1,1], [0,1,2],[0,1,3],[0,1,4]],\n",
    "                    hypothesis2, [1,2,4])\n",
    "t2 = tf.SparseTensor([[0,0,0], [0,0,1],[0,0,2],[0,0,3],[0,0,4], [0,0,5], \n",
    "                      [0,1,0],[0,1,1],[0,1,2],[0,1,3],[0,1,4],[0,1,5]],\n",
    "                    truth2, [1,2,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(session.run(tf.edit_distance(h2,t2,normalize=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.6666667  0.16666667]]\n"
     ]
    }
   ],
   "source": [
    "print(session.run(tf.edit_distance(h2,t2,normalize=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis_words = [\"casa\", \"casita\", \"caseron\", \"tensor\", \"python\"]\n",
    "truth_word = \"algoritmo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0],\n",
       " [0, 0, 1],\n",
       " [0, 0, 2],\n",
       " [0, 0, 3],\n",
       " [1, 0, 0],\n",
       " [1, 0, 1],\n",
       " [1, 0, 2],\n",
       " [1, 0, 3],\n",
       " [1, 0, 4],\n",
       " [1, 0, 5],\n",
       " [2, 0, 0],\n",
       " [2, 0, 1],\n",
       " [2, 0, 2],\n",
       " [2, 0, 3],\n",
       " [2, 0, 4],\n",
       " [2, 0, 5],\n",
       " [2, 0, 6],\n",
       " [3, 0, 0],\n",
       " [3, 0, 1],\n",
       " [3, 0, 2],\n",
       " [3, 0, 3],\n",
       " [3, 0, 4],\n",
       " [3, 0, 5],\n",
       " [4, 0, 0],\n",
       " [4, 0, 1],\n",
       " [4, 0, 2],\n",
       " [4, 0, 3],\n",
       " [4, 0, 4],\n",
       " [4, 0, 5]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_h_words = len(hypothesis_words)\n",
    "h_idx = [[xi,0,yi] for xi,x in enumerate(hypothesis_words) for yi,y in enumerate(x)]\n",
    "h_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c',\n",
       " 'a',\n",
       " 's',\n",
       " 'a',\n",
       " 'c',\n",
       " 'a',\n",
       " 's',\n",
       " 'i',\n",
       " 't',\n",
       " 'a',\n",
       " 'c',\n",
       " 'a',\n",
       " 's',\n",
       " 'e',\n",
       " 'r',\n",
       " 'o',\n",
       " 'n',\n",
       " 't',\n",
       " 'e',\n",
       " 'n',\n",
       " 's',\n",
       " 'o',\n",
       " 'r',\n",
       " 'p',\n",
       " 'y',\n",
       " 't',\n",
       " 'h',\n",
       " 'o',\n",
       " 'n']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_chars = list(''.join(hypothesis_words))\n",
    "h_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "h3 = tf.SparseTensor(h_idx, h_chars, [num_h_words, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'algoritmoalgoritmoalgoritmoalgoritmoalgoritmo'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truth_word_vect = [truth_word]*num_h_words\n",
    "truth_word_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0],\n",
       " [0, 0, 1],\n",
       " [0, 0, 2],\n",
       " [0, 0, 3],\n",
       " [0, 0, 4],\n",
       " [0, 0, 5],\n",
       " [0, 0, 6],\n",
       " [0, 0, 7],\n",
       " [0, 0, 8],\n",
       " [1, 0, 0],\n",
       " [1, 0, 1],\n",
       " [1, 0, 2],\n",
       " [1, 0, 3],\n",
       " [1, 0, 4],\n",
       " [1, 0, 5],\n",
       " [1, 0, 6],\n",
       " [1, 0, 7],\n",
       " [1, 0, 8],\n",
       " [2, 0, 0],\n",
       " [2, 0, 1],\n",
       " [2, 0, 2],\n",
       " [2, 0, 3],\n",
       " [2, 0, 4],\n",
       " [2, 0, 5],\n",
       " [2, 0, 6],\n",
       " [2, 0, 7],\n",
       " [2, 0, 8],\n",
       " [3, 0, 0],\n",
       " [3, 0, 1],\n",
       " [3, 0, 2],\n",
       " [3, 0, 3],\n",
       " [3, 0, 4],\n",
       " [3, 0, 5],\n",
       " [3, 0, 6],\n",
       " [3, 0, 7],\n",
       " [3, 0, 8],\n",
       " [4, 0, 0],\n",
       " [4, 0, 1],\n",
       " [4, 0, 2],\n",
       " [4, 0, 3],\n",
       " [4, 0, 4],\n",
       " [4, 0, 5],\n",
       " [4, 0, 6],\n",
       " [4, 0, 7],\n",
       " [4, 0, 8]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_idx = [[xi,0,yi] for xi,x in enumerate(truth_word_vect) for yi, y in enumerate(x)]\n",
    "t_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_chars = list(''.join(truth_word_vect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3 = tf.SparseTensor(t_idx, t_chars, [num_h_words,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.]\n",
      " [7.]\n",
      " [8.]\n",
      " [8.]\n",
      " [8.]]\n"
     ]
    }
   ],
   "source": [
    "print(session.run(tf.edit_distance(h3, t3, normalize=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.       ]\n",
      " [0.7777778]\n",
      " [0.8888889]\n",
      " [0.8888889]\n",
      " [0.8888889]]\n"
     ]
    }
   ],
   "source": [
    "print(session.run(tf.edit_distance(h3, t3, normalize=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sparse_words_vect(word_list):\n",
    "    num_words = len(word_list)\n",
    "    idx = [[xi,0,yi] for xi,x in enumerate(word_list) for yi, y in enumerate(x)]\n",
    "    chars = list(''.join(word_list))\n",
    "    return tf.SparseTensorValue(idx, chars, [num_words, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_string_sp = create_sparse_words_vect(hypothesis_words)\n",
    "tr_string_sp = create_sparse_words_vect([truth_word]*len(hypothesis_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_input = tf.sparse_placeholder(dtype=tf.string)\n",
    "t_input = tf.sparse_placeholder(dtype=tf.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "edit_distance = tf.edit_distance(h_input, t_input, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_dict = {h_input:hyp_string_sp, t_input:tr_string_sp}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.       ]\n",
      " [0.7777778]\n",
      " [0.8888889]\n",
      " [0.8888889]\n",
      " [0.8888889]]\n"
     ]
    }
   ],
   "source": [
    "print(session.run(edit_distance, feed_dict=feed_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Otras distancias para comparar palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distancia de Hamming\n",
    "- Número de caracteres iguales en la misma posición. \n",
    "- Las dos palabras deben ser de la misma longitud\n",
    "$$D(s_1,s_2) = \\sum_{i=1}^n I_i$$\n",
    "donde $I_i=1$ si las dos palabras tienen el mismo caracter en la posición i-ésima  y 0 en otro caso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distancia del Coseno\n",
    "- Obtenemos el producto escalar del vector de k-gramas de cada palabra dividida por la norma dos de los mismos\n",
    "$$D(s_1,s_2) = 1 - \\frac{k(s_1)\\cdot k(s_2)}{||k(s_1)||||k(s_2)||}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distancia de Jaccard\n",
    "- Numero de caracteres en común entre las dos palabras dividido por la unión total de caracteres de ambas palabras\n",
    "$$D(s_1,s_2) = \\frac{|s_1\\cap s_2|}{|s_1\\cup s_2|}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
